{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YALNzo0pqmYO"
   },
   "source": [
    "<b>`Read 'README.TXT'. You need to install packages in requirements.txt first.`</b> <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPngDNr4qmYR"
   },
   "source": [
    "# Was that Tweet Posted by a Republican?\n",
    "Can you tell whether a tweet was posted by a republican? You only have access to the text (tweet) that an unknown person wrote, your job is to tell whether that unknown person was a republican (Trump, Pence or GOP) or a democrat? This is exactly what we'll do in this notebook. \n",
    "\n",
    "## Preprocessing Raw Tweets and Feature Extraction:\n",
    "For those who have some familiarity with Machine Learning, note that preprocessing of data and feature extraction are parts of full Machine Learning ML [pipeline](https://hub.packtpub.com/automl-build-machine-learning-pipeline-tutorial/). The first part of this notebook will teach you and test you on `preprocessing` data in the context of 'Natural Language Processing' using the classic NLP library [NLTK](https://www.nltk.org). For example, in class you learned and discussed enthusiastically [stemming](https://en.wikipedia.org/wiki/Stemming) and [lemmatizing](https://en.wikipedia.org/wiki/Lemmatisation) with sir Asim. Now, you'll get hands on experience with those processes and other nitty gritty similar stuff that comes in handy in Natural Language Processing. <br>In the second part of this notebook, you'll convert that preprocessed data into TF-IDF Matrix using another famous library in Machine Learning called [sklearn](https://scikit-learn.org/stable/)- instead of implementing TF-IDF from scratch. \n",
    "\n",
    "\n",
    "You will be preprocessing the Twitter data we extracted using [this](https://dev.twitter.com/overview/api) api. We extracted the tweets posted by the following six Twitter accounts: `realDonaldTrump, mike_pence, GOP, HillaryClinton, timkaine, TheDemocrats`. Note that `Data Extraction` is also a part of an ML Pipeline which we have already done for you for this notebook. Data Extraction can sometimes be the most challenging part though. \n",
    "\n",
    "For every tweet, we collected two pieces of information:\n",
    "- `screen_name`: the Twitter handle of the user tweeting and\n",
    "- `text`: the content of the tweet.\n",
    "\n",
    "We divided the tweets into two parts - the [train and test sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).  The training set contains both the `screen_name` and `text` of each tweet; the test set only contains the `text`. Have a look at the dataset in files called `tweets_train.csv.gz` and `tweets_test.csv.gz`.\n",
    "\n",
    "\n",
    "The overarching goal of the problem is to infer the political inclination (whether **R**epublican or **D**emocratic) of the author from the tweet text. The ground truth (i.e., true class labels) are determined from the `screen_name` of the tweet as follows:\n",
    "- **R**: `realDonaldTrump, mike_pence, GOP`\n",
    "- **D**: `HillaryClinton, timkaine, TheDemocrats`\n",
    "\n",
    "We can treat this as a binary classification problem. We'll follow this common structure to tackling this problem:\n",
    "\n",
    "1. **preprocessing**: clean up the raw tweet text using the various functions offered by [the Natural Language Toolkit (`nltk`)](http://www.nltk.org/genindex.html).\n",
    "2. **features**: construct bag-of-words feature vectors- [TFIDF](https://en.wikipedia.org/wiki/Tf‚Äìidf) matrix.\n",
    "3. **classification**: learn a binary classification model using [`scikit-learn`](http://scikit-learn.org/stable/modules/classes.html). \n",
    "\n",
    "Note that `nltk` supports optional corpora, toy grammars, trained models, etc. For this assignment, you have to manually install the stopwords list and `WordNetLemmatizer`. We'll begin by installing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "322Df15rqmYS",
    "outputId": "6781d140-fb51-4257-a398-fa5441147335",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sam/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING nltk_download: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import string\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gzip\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from testing.testing import test\n",
    "\n",
    "def nltk_download_test(nltk_download):\n",
    "    nltk_download()\n",
    "    try:\n",
    "        lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        test.true(lemmatizer is not None)\n",
    "        stopwords=nltk.corpus.stopwords.words('english')\n",
    "        test.true(stopwords is not None)\n",
    "    except LookupError:\n",
    "        test.true(False)\n",
    "        \n",
    "@test\n",
    "def nltk_download():\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpzjMK4PqmYX"
   },
   "source": [
    "## 1. Text Processing\n",
    "\n",
    "You first task to fill in the following function which processes and tokenizes raw text. The tokens must:\n",
    "\n",
    "1. be in lower case.\n",
    "2. appear in the same order as in the raw text.\n",
    "3. be in their lemmatized form, if one exists. If a word cannot be lemmatized, do not include it in the output.\n",
    "4. **not** contain any characters other than numbers and digits; you should:\n",
    "   1. remove trailing `'s`: `Children's` becomes `children`\n",
    "   2. omit other apostrophes: `don't` becomes `dont`\n",
    "   3. break tokens at other punctuation and/or unicode characters: `word-of-mouth` becomes `word`, `of`, `mouth` \n",
    "5. if the lemmatized form is a stopword, it should not appear in the output\n",
    "6. not include the parts of any t.co urls. Many tweets contain URLs from the domain `t.co`; you should strip all such URLs.\n",
    "\n",
    "If you figure out the right order to perform these operations, solving this problem is much easier.\n",
    "\n",
    "`Aside`, We should emphasize the power of regular expressions (regex). You certainly had fun in quiz 1 with regex. And Python does have a regex [library](https://docs.python.org/2/library/re.html) for you. Use them whenever you can. A beautiful line of regex can replace a chunk of ugly code. Once you've sufficient experience with regex, only then you'll really appreciate its utility. <br>\n",
    "**Stopwords** are words that appear very often in text, usually playing a grammatical role (\"and\", \"a\", etc.). When comparing text similarity, these are not very useful; so we eliminate them at this stage. (NLTK provides us with a list of stopwords for English, which we will use later.)\n",
    "\n",
    "Hints:\n",
    "\n",
    " - you should use `nltk.word_tokenize()` in your solution\n",
    " - you should break tokens at all characters that are not in `string.ascii_letters` or `string.digits`\n",
    " - test your URL stripping! It's very easy to make a mistake with it.\n",
    " - When lemmatizing, you should convert the output to `str` (i.e. `lemmatized_word = str(lemmatizer.lemmatize(word))`) because the lemmatizer may return a non-string object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAIOOHnKqmYY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING preprocess: PASSED 23/23\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_test(preprocess):\n",
    "    \n",
    "    test.equal(preprocess(\"I'm doing well! How about you?\"), ['im', 'doing', 'well', 'how', 'about', 'you'])\n",
    "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\"),    ['education', 'is', 'the', 'ability', 'to', 'listen', 'to', 'almost', 'anything', 'without', 'losing', 'your', 'temper', 'or', 'your', 'self', 'confidence'])\n",
    "    \n",
    "    # Punctuation and space handling\n",
    "    test.equal(preprocess(\" a..a. .a . a.\"), ['a', 'a', 'a', 'a'])\n",
    "    test.equal(preprocess(\"word-of-mouth self-esteem\"), ['word', 'of', 'mouth', 'self', 'esteem'])\n",
    "    \n",
    "    # Apostrophe handling\n",
    "    test.equal(preprocess(\"you've\"), ['youve'])\n",
    "    test.equal(preprocess(\"She's\"), ['she'])\n",
    "    test.equal(preprocess(\"SHE'S\"), ['she'])\n",
    "    test.equal(preprocess(\"Cea'sar\"), ['ceaar']) # You can assume that there are no mid-word \"'s\" substrings.\n",
    "    \n",
    "    # Lemmatizer\n",
    "    test.equal(preprocess(\"walks\"), ['walk'])\n",
    "    \n",
    "    # Stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    test.equal(preprocess(\"I'm doing well! How about you?\", stopwords), ['im', 'well'])\n",
    "    test.equal(preprocess(\"Education is the ability to listen to almost anything without losing your temper or your self-confidence.\", stopwords), ['education', 'ability', 'listen', 'almost', 'anything', 'without', 'losing', 'temper', 'self', 'confidence'])\n",
    "    \n",
    "    # Unicode handling\n",
    "    test.equal(preprocess(\"dootüëèdoot\"), [\"doot\", \"doot\"])\n",
    "    \n",
    "\n",
    "    \n",
    "    # From the training set:\n",
    "    SW=set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"])\n",
    "    test.equal(preprocess('RT @GOPconvention: #Oregon votes today. That means 62 days until the @GOPconvention! https://t.co/OoH9FVb7QS', stopwords=SW), ['gopconvention', 'oregon', 'vote', 'today', 'mean', '62', 'day', 'gopconvention'])\n",
    "    test.equal(preprocess('RT @DWStweets: The choice for 2016 is clear: We need another Democrat in the White House. #DemDebate #WeAreDemocrats http://t.co/0n5g0YN46f', stopwords=SW), ['dwstweets', 'choice', '2016', 'clear', 'need', 'another', 'democrat', 'white', 'house', 'demdebate', 'wearedemocrats'])\n",
    "    test.equal(preprocess(\"Trump's calling for trillion dollar tax cuts for Wall Street.\\n\\nIt's time for them to pay their fair share. https://t.co/y8vyESIOES\", stopwords=SW), ['trump', 'calling', 'trillion', 'dollar', 'tax', 'cut', 'wall', 'street', 'time', 'pay', 'fair', 'share'])\n",
    "    test.equal(preprocess(\".@TimKaine's guiding principle: the belief that you can make a difference through public service. https://t.co/YopSUeMqOX\", stopwords=SW), ['timkaine', 'guiding', 'principle', 'belief', 'make', 'difference', 'public', 'service'])\n",
    "    test.equal(preprocess('Glad the Senate could pass a #THUD / MilCon / VetAffairs approps bill with solid provisions for Virginia: https://t.co/NxIgRC3hDi', stopwords=SW), ['glad', 'senate', 'could', 'pas', 'thud', 'milcon', 'vetaffairs', 'approps', 'bill', 'solid', 'provision', 'virginia'])\n",
    "    test.equal(preprocess('RT @IndyThisWeek: An @rtv6 exclusive: @GovPenceIN sits down with @RafaelOnTV\\nSee it Sunday morning at 8:30a on RTV6 and our RTV6 app. http:‚Ä¶', stopwords=SW), ['indythisweek', 'rtv6', 'exclusive', 'govpencein', 'sits', 'rafaelontv', 'see', 'sunday', 'morning', '8', '30a', 'rtv6', 'rtv6', 'app'])\n",
    "    test.equal(preprocess('From Chatham Town Council to Congress, @RepRobertHurt has made a strong mark on his community. Proud of our work together on behalf of VA!', stopwords=SW), ['chatham', 'town', 'council', 'congress', 'reproberthurt', 'ha', 'made', 'strong', 'mark', 'community', 'proud', 'work', 'together', 'behalf', 'va'])\n",
    "    test.equal(preprocess('Thank you New Orleans, Louisiana!\\n#MakeAmericaGreatAgain #VoteTrump\\nhttps://t.co/tI1h9xT9GX https://t.co/0bf7BOlWEj', stopwords=SW), ['thank', 'new', 'orleans', 'louisiana', 'makeamericagreatagain', 'votetrump'])\n",
    "\n",
    "    # URL handling\n",
    "    test.equal(preprocess(\"http://t.co/WJs5bmRthU,https://t.co/WJs5bmRthU,\"), [])   \n",
    "    test.equal(preprocess(\"boohttp://t.co/WJs5bmRthUhello\"), [\"boo\", \"hello\"])\n",
    "    test.equal(preprocess(\"https://t.co/ZhEy√Ñ¬∂aaaa\"), ['http', 't', 'co', 'zhey', 'aaaa'])\n",
    "    \n",
    "    \n",
    "@test\n",
    "def preprocess(text, stopwords={}, lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \"\"\" Normalizes case and handles punctuation\n",
    "    \n",
    "    args:\n",
    "        text: str -- raw text\n",
    "        stopwords : Set[str] -- lemmatized tokens to exclude from the output\n",
    "        lemmatizer : Lemmatizer -- an instance of a class implementing the lemmatize() method\n",
    "\n",
    "    Outputs:\n",
    "        list(str): tokenized text\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "   \n",
    "    text=re.sub('https?://t\\.co/[A-Za-z0-9]{10}',' ',text)\n",
    "    text=text.lower()\n",
    "    text=re.sub('[\\,\\!\\?\\#\\:\\/\\.\\-]',' ',text)\n",
    "    text=re.sub('(\\'s)|(\\')','',text)\n",
    "    for char in text:\n",
    "        if char not in string.ascii_letters and char not in string.digits:\n",
    "            text=text.replace(char,' ')\n",
    "    tokenized=word_tokenize(text)\n",
    "    lemmatized=[]\n",
    "    for word in tokenized:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "    final=[word for word in lemmatized if word not in stopwords]\n",
    "\n",
    "                \n",
    "    return final\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zz9CBW44qmYa"
   },
   "source": [
    "We give you some code that uses `preprocess` to prepare the data. This should take no more than 6s to run; if it takes longer than that, you need to make your preprocessing function run quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44JQU88bqmYb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING read_data: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%timeit\n",
    "## Uncomment the previous line to time your code. Remember to comment it out before uploading your solution.\n",
    " \n",
    "def read_data_test(read_data):\n",
    "    data_train, data_test = read_data()\n",
    "    \n",
    "    test.equal(len(data_train), 17298)\n",
    "    test.equal(len(data_test), 1000)\n",
    "    \n",
    "    #print(data_train[:8])\n",
    "\n",
    "def read_csv(stem, process=lambda x: x):\n",
    "    with gzip.open(f\"{stem}.csv.gz\", \"rt\", newline='', encoding=\"UTF-8\") as file:\n",
    "        csvr = csv.reader(file)\n",
    "        next(csvr)\n",
    "        return list(map(process, csvr))\n",
    "\n",
    "def is_republican(r):\n",
    "    return r in [\"realDonaldTrump\", \"mike_pence\", \"GOP\"]\n",
    "\n",
    "@test\n",
    "def read_data(extra_stopwords=set()):\n",
    "    \"\"\"Reads the dataset from the csv.gz files\n",
    "    \n",
    "    return : Tuple[data_train, data_test]\n",
    "        data_train : List[Tuple[is_republican, tokenized_tweet]]\n",
    "            is_republican : bool -- True if tweet is from a republican\n",
    "            tokenized_tweet : List[str] -- the tweet, tokenized by preprocess()\n",
    "    \"\"\"\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english')) | set([\"http\", \"co\", \"rt\", \"amp\"]) | extra_stopwords\n",
    "    data_train = read_csv(\"tweets_train\", process=lambda r: (is_republican(r[0]), preprocess(r[1], stopwords)))\n",
    "    data_test = read_csv(\"tweets_test\", process=lambda r: preprocess(r[0], stopwords))\n",
    "    \n",
    "    return (data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RV9_1bY3qmYe"
   },
   "source": [
    "## 2. Feature Construction\n",
    "\n",
    "The next step is to derive feature vectors from the tokenized tweets. In this section, you will be constructing a bag-of-words [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature vector.\n",
    "\n",
    "The number of possible words is prohibitively large, and not all words are useful for our task. We will begin by filtering the vectors using a common heuristic:\n",
    "\n",
    "We calculate a frequency distribution of words in the corpus, and remove words at the head (most frequent) and tail (least frequent) of the distribution. Most frequently used words (often called stopwords) provide very little information about the similarity of two pieces of text; we have already removed these. Words with extremely low frequency tend to be typos.\n",
    "\n",
    "We will now implement a function which counts the number of times that each token is used in the training corpus. You should return a [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter) object with the number of times that each word appears in the dataset.\n",
    "\n",
    "(This should take no more than 20s to run, including reading the files.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzaGnscUqmYf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING get_distribution: PASSED 10/10\n",
      "###\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_distribution_test(get_distribution):\n",
    "    data_train, data_test = read_data()\n",
    "    dist = get_distribution(data_train)\n",
    "    test.true(isinstance(dist, collections.Counter))\n",
    "    if dist is None:\n",
    "        return\n",
    "\n",
    "    # Simple test cases:\n",
    "    test.equal(dist['trump'], 1812)\n",
    "    test.equal(dist['clinton'], 1107)\n",
    "    test.equal(dist['president'], 788)\n",
    "    test.equal(dist['american'], 745)\n",
    "    test.equal(dist['job'], 676)\n",
    "    test.equal(dist['obama'], 438)\n",
    "    test.equal(dist['hoosier'], 393)\n",
    "\n",
    "    # Check that you have the correct number of unique words:\n",
    "    test.equal(len(dist), 16762)\n",
    "    # There are 8048 words used exactly once, 2399 words used twice, etc:\n",
    "    test.equal(collections.Counter(dist.values()).most_common(5), [(1, 8048), (2, 2399), (3, 1198), (4, 778), (5, 577)])\n",
    "    \n",
    "    plt.hist(dist.values(), bins=100)\n",
    "    plt.yscale('log')\n",
    "\n",
    "@test\n",
    "def get_distribution(data_train):\n",
    "    \"\"\" Calculates the word count distribution, excluding stopwords.\n",
    "\n",
    "    args: \n",
    "        data_train -- the training data\n",
    "\n",
    "    return : collections.Counter -- the distribution of word counts\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    lst=[]\n",
    "    for tup in data_train:\n",
    "        lst+=tup[1]\n",
    "    words=nltk.corpus.stopwords.words('english')\n",
    "    lst=[i for i in lst if i not in words]\n",
    "    return nltk.FreqDist(lst)\n",
    "    \n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpRW7xyEqmYi"
   },
   "source": [
    "Notice the distribution looks exponential, even with a logarithmic y-axis; there are a lot words that appear only once. Lets figure out what these words are so we can eliminate them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3rctP8MqmYj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING get_rare_words: PASSED 3/3\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_rare_words_test(get_rare_words):\n",
    "    data_train, data_test = read_data()\n",
    "    dist = get_distribution(data_train)\n",
    "    new_stopwords = get_rare_words(dist)\n",
    "    \n",
    "    with open(\"get_rare_words.output\", \"rt\", newline=\"\") as f:\n",
    "        ref_stopwords = set(t.strip() for t in f)\n",
    "\n",
    "    test.true(isinstance(new_stopwords, set))\n",
    "    # Extra words in your solution:\n",
    "    test.equal(new_stopwords - ref_stopwords, set())\n",
    "    # Words missing from your solution:\n",
    "    test.equal(ref_stopwords - new_stopwords, set())\n",
    "\n",
    "@test\n",
    "def get_rare_words(dist):\n",
    "    \"\"\"use the word count information from the training data to find more stopwords\n",
    "\n",
    "    args:\n",
    "        dist: collections.Counter -- the output of get_distribution\n",
    "\n",
    "    returns : Set[str] -- a set of all words that appear exactly once in the training data\n",
    "    \"\"\"\n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    lst=[]\n",
    "    lst1=[]\n",
    "    for word in dist:\n",
    "        if dist[word] == 1:\n",
    "            lst.append(word)\n",
    "            \n",
    "    lst1=[i for i in lst1 if i not in lst]\n",
    "    return set(lst)\n",
    "    \n",
    "\n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TUzRRESLqmYn"
   },
   "source": [
    "Here we provide a wrapper function to cache the preprocessed data. This helps it not take quite as long to re-run. If you change anything above this cell, re-run this cell to clear the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lzBDHYwKqmYo"
   },
   "outputs": [],
   "source": [
    "global PREPROCESSED_DATA_CACHE\n",
    "PREPROCESSED_DATA_CACHE = None\n",
    "\n",
    "def get_data():\n",
    "    global PREPROCESSED_DATA_CACHE\n",
    "    if PREPROCESSED_DATA_CACHE is None:\n",
    "        data_train, data_test = read_data()\n",
    "        dist = get_distribution(data_train)\n",
    "        new_stopwords = get_rare_words(dist)\n",
    "        PREPROCESSED_DATA_CACHE = read_data(new_stopwords)\n",
    "\n",
    "    return PREPROCESSED_DATA_CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wNnPYrPqmYr"
   },
   "source": [
    "### Vectorizing\n",
    "\n",
    "Now we have each tweet as a list of words, excluding words with high- and low-frequencies. We want to convert these into a [sparse](https://en.wikipedia.org/wiki/Sparse_matrix) feature matrix, where each row corresponds to a tweet and each column to a possible word. Odds are you have not seen TFIDF before; You may also find supplementary notes helpful. However, you'll not implement TFIDF from scratch rather use a library function. We can use [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) of [`scikit-learn`](https://scikit-learn.org/stable/) to do this quite easily). \n",
    "\n",
    "There's a catch, though: `TfidfVectorizer` expects the input to be a string, and (by default) it perfoms its own analyzing. You have to override that behavior by passing in `do_nothing` to the constructor as an optional parameter.\n",
    "\n",
    "Hints:\n",
    "\n",
    " - Read [the documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes) carefully, and then this [blog post](http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/) ([mirror](http://archive.is/pVdqE)). You need to pass in `do_nothing` in two locations.\n",
    " - You should use only the training data to `fit` or `fit_transform` the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqZJE50nqmYs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING create_features: PASSED 5/5\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Helper function, do not change:\n",
    "def do_nothing(x):\n",
    "    return x\n",
    "\n",
    "def create_features_test(create_features):\n",
    "    train_features, train_labels, test_features = create_features(*get_data())\n",
    "\n",
    "    test.equal(repr(train_features), \"\"\"<17298x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 161480 stored elements in Compressed Sparse Row format>\"\"\")\n",
    "\n",
    "    test.equal(repr(test_features), \"\"\"<1000x8714 sparse matrix of type '<class 'numpy.float64'>'\n",
    "\twith 9037 stored elements in Compressed Sparse Row format>\"\"\")\n",
    "\n",
    "    test.equal(train_labels.dtype, bool)\n",
    "    test.equal(len(train_labels), 17298)\n",
    "    test.equal(sum(train_labels), 8646)\n",
    "\n",
    "@test\n",
    "def create_features(train_data, test_data):\n",
    "    \"\"\"creates the feature matrices and label vector for the training and test sets.\n",
    "\n",
    "    args:\n",
    "        train_data : List[Tuple[is_republican, tweet_words]]\n",
    "            is_republican : bool -- True if Republican, False otherwise\n",
    "            tweet_words : List[str] -- the processed tweet tokens\n",
    "        test_data : List[List[str]] -- a list of processed tweets\n",
    "\n",
    "    returns: Tuple[train_features, train_labels, test_features]\n",
    "        train_features : scipy.sparse.csr.csr_matrix -- feature matrix for the training set\n",
    "        train_labels : np.array[num_train] -- a numpy vector, where 1 stands for Republican and 0 stands for Democrat \n",
    "        test_features : scipy.sparse.csr.csr_matrix -- feature matrix for the test set\n",
    "    \"\"\"\n",
    "\n",
    "    train_labels = None\n",
    "    train_features = None\n",
    "    test_features = None\n",
    "    \n",
    "    ## ***** START OF YOUR CODE **** ##\n",
    "    lst1=[]\n",
    "    lst=[]\n",
    "    for tup in train_data:\n",
    "        lst1.append(tup[0])\n",
    "        lst.append(tup[1])\n",
    "    train_labels=np.array(lst1)\n",
    "    tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=do_nothing,\n",
    "    preprocessor=do_nothing,\n",
    "    token_pattern=None)\n",
    "    tfidf.fit(lst)\n",
    "    train_features=tfidf.transform(lst)\n",
    "    test_features=tfidf.transform(test_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## ***** END OF YOUR CODE ***** ##\n",
    "\n",
    "    return (train_features, train_labels, test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJIrmRDqqmYu"
   },
   "source": [
    "Observe that the created matrices are very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL36zG6LqmYv"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfcjGXU3qmYw"
   },
   "source": [
    "We can now use these features to predict whether whether a tweet was posted by a republican. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EYIjFMJqmYx"
   },
   "source": [
    "## 3. Classification\n",
    "\n",
    "We are ready to put it all together and train the classification model.\n",
    "\n",
    "You will be will be using the Support Vector Machine [`sklearn.svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC). [Here](http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html) is a quick introduction to SVMs.\n",
    "\n",
    "At the heart of an SVM is the concept of a _kernel function_, which determines the distance between two data points. `sklearn.svm.SVC` natively supports four kernel functions: `linear`, `poly`, `rbf`, `sigmoid`. For this problem space, we will use the `linear` kernel.\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. build a classifier using the `linear` kernel,\n",
    "2. train it using the training set,\n",
    "3. evaluate the trained model on the training set, and then\n",
    "4. use it to predict classification on our test set.\n",
    "\n",
    "Let's begin by training a classifier. This should take no more than 20s to run. You should set the optional parameter `gamma` to `auto`, but leave the rest at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ye7bKB9qmYx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.svm._classes.SVC'>\n",
      "### TESTING learn_classifier: PASSED 1/1\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "def learn_classifier_test(learn_classifier):\n",
    "    train_features, train_labels, _ = create_features(*get_data())\n",
    "    classifier = learn_classifier(train_features, train_labels)\n",
    "\n",
    "    test.equal(repr(classifier).replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\"  \", \" \"), \"\"\"SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\"\"\")\n",
    "\n",
    "@test\n",
    "def learn_classifier(train_features, train_labels, kernel=\"linear\"):\n",
    "    \"\"\"learns a classifier from the input features and labels using a specified kernel function\n",
    "\n",
    "    args:\n",
    "        train_features: scipy.sparse.csr.csr_matrix -- sparse matrix of features\n",
    "        train_labels : numpy.ndarray(bool): binary vector of class labels\n",
    "        kernel : str -- kernel function to be used with classifier, must be (linear|poly|rbf|sigmoid)\n",
    "\n",
    "    return : sklearn.svm.classes.SVC -- classifier\n",
    "    \"\"\"\n",
    "\n",
    "    assert kernel in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "    clf = SVC(gamma='auto',kernel='linear')\n",
    "    predictor=clf.fit(train_features, train_labels)\n",
    "    print(type(predictor))\n",
    "    return predictor\n",
    "\n",
    "     # Implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GvsxpblxqmY0"
   },
   "source": [
    "Now that we know how to train a classifier, the next step is to measure its performance. In general, this step is necessary to select the best model among a given set of models, or even tune [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) for a given model.\n",
    "\n",
    "We would ordinarily use a held-out validation set to evaluate the performance of the classifier. The use of a held-out set prevents overfitting to the data, and you will do this for another assignment. For this problem, though, we can use the training set.\n",
    "To measure classification accuracy we should use the [$F_1$ score](https://en.wikipedia.org/wiki/F1_score). But we won't ask you to implement or learn $F_1$ score for this notebook. However, Feel free to play with sklearn library [functions](https://scikit-learn.org/stable/modules/model_evaluation.html). \n",
    "You'll find [score](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.score) useful to find accuracy, and [predict](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.predict) to find labels of your tweets. Consider this part open-ended and get as many insights as you wish. This area will be explored in a future assignment further. For now, you've learned [so much!](https://www.google.com/imgres?imgurl=https%3A%2F%2Fi.ytimg.com%2Fvi%2FfhA5GIx51Kg%2Fmaxresdefault.jpg&imgrefurl=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DfhA5GIx51Kg&tbnid=r_rUj2s8BRUUXM&vet=12ahUKEwjUiLCv9KznAhUUPhoKHXKBBOsQMygBegUIARDsAQ..i&docid=G16Fe5uFaWJtwM&w=1280&h=720&q=schindler%27s%20list%20ending&ved=2ahUKEwjUiLCv9KznAhUUPhoKHXKBBOsQMygBegUIARDsAQ). Well Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPxc0WSCqmY3"
   },
   "outputs": [],
   "source": [
    "## Open-Ended Part:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "Political_Inclination_from_Tweets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
